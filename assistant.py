import requests
from datetime import datetime
from collections import defaultdict

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.utilities import WikipediaAPIWrapper


# ----------------------------
# Tools
# ----------------------------

wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)

def normalize_fact_query(query: str) -> str:
    q = query.lower()

    if "warmest country in europe" in q:
        return "Warmest countries in Europe"

    if "coldest country in europe" in q:
        return "Coldest countries in Europe"

    return query

def wikipedia_lookup(query: str) -> str:
    try:
        return wiki.run(query)
    except Exception as e:
        return f"Wikipedia error: {e}"


def now() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# ----------------------------
# Router
# ----------------------------

def route(query: str) -> str:
    q = query.lower().strip()

    # 1Ô∏è‚É£ Deterministic tools
    if any(word in q for word in ["time", "date", "day"]):
        return "time"

    # 2Ô∏è‚É£ Comparative / ranking questions
    if any(word in q for word in [
        "warmest", "coldest", "largest", "smallest",
        "highest", "lowest", "longest", "shortest",
        "best", "worst"
    ]):
        return "comparison"

    # 3Ô∏è‚É£ Entity lookups (Wikipedia-safe)
    if q.startswith((
        "who is",
        "what is",
        "where is",
        "tell me about",
        "define"
    )):
        return "entity"

    # 4Ô∏è‚É£ Everything else
    return "open"

# ----------------------------
# Assistant
# ----------------------------

class Assistant:
    def __init__(self, model="llama3.2"):
        self.llm = OllamaLLM(model=model, temperature=0.7)

        self.memory = defaultdict(list)

        self.system_prompt = "You are a helpful, concise assistant."


    def run(self, query: str, session_id: str = "default") -> str:
        intent = route(query)

    # --- 1Ô∏è‚É£ Time ---
        if intent == "time":
            return f"‚è∞ {now()}"

    # --- 2Ô∏è‚É£ Entity lookup (Wikipedia) ---
        if intent == "entity":
            result = wikipedia_lookup(query)
            return f"üìö Source: Wikipedia\n\n{result}"

    # --- 3Ô∏è‚É£ Comparative reasoning (LLM, constrained) ---
        if intent == "comparison":
            prompt = ChatPromptTemplate.from_messages([
                ("system",
                "You are a factual assistant. "
                "Answer clearly and concisely. "
                "Do NOT mention movies, books, or unrelated topics. "
                "If multiple answers exist, explain briefly."),
                ("user", query),
            ])

            chain = prompt | self.llm
            response = chain.invoke({})

            return (
                "‚ÑπÔ∏è Answer generated by local LLM (no external lookup).\n\n"
                + response
            )

    # --- 4Ô∏è‚É£ Open-ended (LLM + memory) ---
        history = self.memory[session_id]

        messages = [("system", self.system_prompt)]

        for turn in history:
            messages.append(("user", turn["user"]))
            messages.append(("assistant", turn["assistant"]))

        messages.append(("user", query))

        prompt = ChatPromptTemplate.from_messages(messages)
        chain = prompt | self.llm
        response = chain.invoke({})

        history.append({
            "user": query,
            "assistant": response
        })

        return response


# ----------------------------
# CLI
# ----------------------------

if __name__ == "__main__":
    assistant = Assistant()
    print("ü§ñ Simple AI Assistant (type 'exit')\n")

    while True:
        q = input("You: ").strip()
        if q.lower() in {"exit", "quit"}:
            break
        print("\nü§ñ", assistant.run(q), "\n")
